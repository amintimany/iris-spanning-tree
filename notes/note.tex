\documentclass[]{scrartcl}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\usepackage{listings}
\usepackage{lstcoq}

\usepackage[dvipsnames]{xcolor}
\usepackage[T1]{fontenc}
\usepackage[scaled=0.85]{beramono}

\usepackage{todonotes}

\usepackage{iris}
\usepackage{mathpartir, pftools}
\usepackage{xfrac}


\newtheorem{lemma}{Lemma}

\lstdefinelanguage[]{MyML}[]{ML}
{% general command to set parameter(s)
basicstyle=\small\ttfamily, % print whole listing small
keywordstyle=\color{blue}\bfseries,
morekeywords=[2]{CAS, true, false, null},
keywordstyle=[2]\bfseries,
identifierstyle=, % nothing happens
commentstyle=, % white comments
stringstyle=\ttfamily, % typewriter type for strings
showstringspaces=false, % no special string spaces
numbers=left,
numberstyle=\tiny,
stepnumber=1,
numbersep=5pt,
escapeinside={(*@}{@*)}
}
\def\MyMLe{\lstinline[language=MyML, basicstyle=\small\ttfamily, mathescape=true]}

\newcommand{\path}{\mathit{path}}
\newcommand{\connected}{\mathit{connected}}
\newcommand{\nodes}{\mathit{nodes}}
\newcommand{\front}{\mathit{front}}
\newcommand{\maximal}{\mathit{maximal}}
\newcommand{\marked}{\mathit{marked}}
\newcommand{\inmem}{\mathit{in\text{-}memory}}
\newcommand{\localgr}{\mathit{own\text{-}graph}}
\newcommand{\globprot}{\mathit{graph\text{-}ctx}}
\newcommand{\globmark}{\mathit{globally\text{-}marked}}
\newcommand{\ismarked}{\mathit{is\text{-}marked}}
\newcommand{\Left}{\mathit{left}}
\newcommand{\Right}{\mathit{right}}
\newcommand{\tree}{\mathit{tree}}
\newcommand{\Null}{\mathit{null}}
\newcommand{\strictSG}{\subseteq_{\mathit{strict}}}

%\newcommand{\dom}{\mathit{dom}}
\newcommand{\None}{\mathit{None}}
\newcommand{\Some}{\mathit{Some}}

%\newcommand{\val}{\mathit{val}}
\newcommand{\locT}{\mathit{loc}}
\newcommand{\option}{\textmon{option}}
\newcommand{\Excl}{\textsf{Excl}}
\newcommand{\excl}{\textmon{excl}}
%\newcommand{\Auth}{\mathit{Auth}}
\newcommand{\Frac}{\textsf{Frac}}
%\newcommand{\finmap}{\overset{\mathit{fin}}{\rightharpoonup}}
\newcommand{\finset}{\textmon{gset}}
\newcommand{\cinvown}{\mathit{cinv\text{-}own}}
\newcommand{\globmr}{\mathit{global\text{-}markings}}
\newcommand{\globgr}{\mathit{global\text{-}graph}}
\newcommand{\gmon}{\textmon{Gmon}}
\newcommand{\graphm}{\textmon{Graph}}
\newcommand{\markings}{\textmon{markings}}
\newcommand{\mrname}{\gamma_{\mathit{mr}}}
\newcommand{\grname}{\gamma_{\mathit{gr}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONCRETE LANGUAGE SYNTAX AND SEMANTICS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\textlang}[1]{\texttt{#1}}
\newcommand{\langkw}[1]{\textlang{\color{blue} #1}}
\newcommand{\lvar}[1]{\textit{#1}} % Yes, this makes language-level variables look like logic-level variables. but we still distinguish locally bound variables from global definitions.
\newcommand{\lvarA}{\lvar{\var}}
\newcommand{\lvarB}{\lvar{\varB}}
\newcommand{\lvarC}{\lvar{\varC}}

\newcommand{\loc}{\ell}

\def\Let#1=#2in{\langkw{let} \spac #1 \mathrel{=} #2 \spac \langkw{in} \spac}
\def\If#1then{\langkw{if} \spac #1 \spac \langkw{then} \spac}
\def\Else{\spac\langkw{else} \spac}
\def\Ref(#1){\langkw{ref}(#1)}
\def\Rec#1 #2={\langkw{rec}\spac{#1}({#2}) \mathrel{=} }
\def\Skip{\langkw{skip}}
\def\Assert{\langkw{assert}}
\newcommand\Inj[1]{\langkw{inj}_{#1}\spac}
\newcommand\Proj[1]{\pi_{#1}\spac}
\def\True{\langkw{true}}
\def\False{\langkw{false}}
\def\Match#1with#2=>#3|#4=>#5end{\langkw{match}\spac#1\spac\langkw{with}\spac#2\Ra#3\mid#4\Ra#5\spac\langkw{end}}
\def\MatchML#1with#2=>#3|#4=>#5end#6{{\arraycolsep=1.4pt\begin{array}[t]{rll}%
    \multicolumn{3}{l}{\langkw{match}\spac#1\spac\langkw{with}}\\%
    &#2&\Ra#3\\|&#4&\Ra#5\\%
    \multicolumn{3}{l}{\langkw{end}#6}%
  \end{array}}}
\def\MatchMLL#1with#2=>#3|#4=>#5|#6=>#7end#8{{\arraycolsep=1.4pt\begin{array}[t]{rll}%
    \multicolumn{3}{l}{\langkw{match}\spac#1\spac\langkw{with}}\\%
    &#2&\Ra#3\\|&#4&\Ra#5\\|&#6&\Ra#7\\%
    \multicolumn{3}{l}{\langkw{end}#8}%
  \end{array}}}
\def\MatchS#1with#2=>#3end{
  \langkw{match}\spac#1\spac\langkw{with}\spac#2\Ra#3\spac\langkw{end}}
\newcommand\CAS{\langkw{CAS}}
\newcommand*\Fork[1]{\langkw{fork}\spac\set{#1}}
\newcommand\deref{\mathop{!}}
\let\gets\leftarrow

\newcommand{\fold}{\langkw{fold}\spac}
\newcommand{\unfold}{\langkw{unfold}\spac}

\newcommand{\Op}[1]{\mathrel{#1}}

\newcommand{\binop}{\circledcirc}
\newcommand{\Plus}{\Op{+}}
\newcommand{\Minus}{\Op{-}}
\newcommand{\Mult}{\Op{*}}
\newcommand{\Eq}{\Op{=}}
\newcommand{\Lt}{\Op{<}}

\newcommand{\TT}{()}

% types
\newcommand{\tvar}{X}
\newcommand{\tvarB}{Y}
\newcommand{\TVar}{\textdom{Tvar}}
\newcommand{\TLam}{\Lambda\spac}

\newcommand{\typ}{\tau}
\newcommand{\typB}{\rho}
\newcommand{\Tunit}{1}
\newcommand{\Tbool}{\mathbb{B}}
\newcommand{\Tnat}{\mathbb{N}}
\newcommand{\Tarr}{\ra}
\def\Tall #1.{\forall #1.\spac}%
\def\Tmu #1.{\mu #1.\spac}%
\def\Tref(#1){\textlang{ref}(#1)}

\newcommand{\Tenv}{\Xi}
\newcommand{\env}{\Gamma}
\newcommand{\typed}[4]{#1 ~|~ #2 \vdash #3 : #4}
\newcommand{\semtyped}[4]{#1 ~|~ #2 \vDash #3 : #4}

\title{Notes on the verification of the concurrent spanning tree algorithm}

\begin{document}
\maketitle

\begin{abstract}
We give a short description of the verification of an
in-place concurrent algorithm for computing
a spanning tree of a graph described below.
\end{abstract}

\begin{figure}
\begin{lstlisting}[mathescape=true, language=MyML]
let try_mark x = CAS(x.mark, false, true)
let unmark_fst x = x.left := null
let unmark_snd x = x.right := null

let rec span x =
  if (x == null) then
    false (*@\label{x-null}@*)
  else
    if try_mark then (*@\label{the-cas}@*)
      let (l, r) =
        (span (x.left) || span (x.right)) in
      if ($\lnot$l) then unmark_fst;
      if ($\lnot$r) then unmark_snd;
      true (*@\label{return-true}@*)
    else
      false (*@\label{cas-fail}@*)
\end{lstlisting}
\caption{The pseudo-code of the spanning tree algorithm in ML style}
\label{fig:code}
\end{figure}

\section{The program}
The algorithm verified is depicted in Figure~\ref{fig:code}.
This algorithm does not compute \emph{the minimum
spanning tree} but rather \emph{a spanning tree}.
In particular, the shape of the computed spanning tree depends on the
scheduling of threads as threads race to mark nodes.
Intuitively, nodes marked by a thread (or its children) form the nodes of
the spanning tree computed by that thread.
Recursively, each thread that manages to mark a node, spawns two
threads to compute spanning trees for the children of that node. 
It then combines those spanning trees to compute a spanning tree with
the node that it has marked at the root.
A thread failing to mark a node will produce an empty
spanning tree.
It can happen for one of the following two reasons.
Either, the node the thread tried to mark does't exist
(the pointer \MyMLe{x} is \MyMLe{null}),
or the node is already marked.
In both cases the parent thread sets its child to \MyMLe{null}.

In the following we first sketch why the algorithm
does what is claimed.
Later we present a proof in Iris which roughly speaking follows the
intuitive argument of Section~\ref{sect:int:reas}.
We finish with a brief comparison with the work by Sergey et al. \cite{Sergey:2015:MVF:2737924.2737964}, who formalized
the correctness of the same algorithm.

\section{Intuitive reasoning}\label{sect:int:reas}
\subsection{Basic definitions}
In this text we assume that every node of the graph has zero, one or two children.
Therefore, we refer to the children of a node as the left or right child (which may not exist for some nodes).
We write $x.\Left = \Null$ and $x.\Right = \Null$ when a node $x$ has no left or right child respectively.
When it is not clear to which graph we are referring we use $g.x.\Left$ and $g.x.\Right$ to refer to the children of a node $x$ in a particular graph $g$.

We write $\path(p, g, x, y)$ to say that $p$ is a path in $g$ from $x$ to $y$.
Here a path $p$ is a sequence
$p = [d_1, \dots, d_n ]$ where $d_1,\dots, d_n \in \{\Left, \Right\}$.

We assume that there is always an empty path from a node to itself.
\[
x \in \nodes(g) \Rightarrow \path([], g, x, x)
\]
A graph is called connected from a node $x$ if for any node in the graph there is a path from $x$ to that node.
\[
\connected(g, x) \eqdef \forall y \in \nodes(g).~ \exists p.~\path(p, g, x, y)
\]
where $\nodes(g)$ is the set of nodes of graph $g$.
\emph{The front} of a set of nodes $A$ in a graph is contained in
a set of nodes $B$ if $A \subseteq \nodes(g)$ and nodes immediately reachable
from nodes in $A$ are all in $B$. We write:
\[
\front(g, A, B) \eqdef A \subseteq \nodes(g) \land
\{x \mid \exists y \in A.~x = y.\Left \lor x = y.\Right \} \subseteq B
\]
A graph is maximal if the front of its nodes is contained in its nodes:
\[
\maximal(g) \eqdef \front(g, \nodes(g), \nodes(g))
\]
A tree is a graph with a unique path to every node:
\[
\tree(g, x) \eqdef \forall y \in \nodes(g), \exists! p.~\path(p, g, x, y)
\]
A graph $g$ is a strict subgraph of $g'$, written $g \strictSG g'$,
if the graph $g$ can be obtained from $g'$ by removing some nodes and children:
\[
\begin{array}{ll}
g \strictSG g' \eqdef & \forall x \in \nodes(g).\\
& (g.x.\Left = \Null \lor g.x.\Left = g'.x.\Left)~\land \\
& (g.x.\Right = \Null \lor g.x.\Right = g'.x.\Right)
\end{array}
\]
A graph $g$ is a spanning tree of a graph $g'$ with root $x$ if
\[
x \in \nodes(g) \land g \strictSG g' \land \tree(g, x) \land \nodes(g) = \nodes(g')
\]
\begin{lemma}\label{lem:in_front_nodes}
\[
\forall A.~ \front(g, A, A) \Rightarrow A = \nodes(g)
\]
\end{lemma}
\subsection{Proof sketch}
Intuitively, the contract (in the Hoare triple style in separation logic) that we would want
to prove for \MyMLe{span} is as follows:
\begin{align*}
& \{\connected(g, x) \star \inmem(g) \star \marked(g) = \emptyset \star x \in \nodes(g)\} \\
& \text{\MyMLe{span}}~x \\
& \{\exists g'.~ \inmem(g') \star \nodes(g') = \nodes(g) \star \tree(g', x), g' \strictSG g \}
\end{align*}
It reads; suppose $g$ is a graph in memory, where no node is marked
(in the memory representation) and $x$ (the argument) is a node of $g$.
Then, after the call
to \MyMLe{span $x$}, there is another graph $g'$, which is a spanning tree of $g$ with root $x$.

To prove that this Hoare triple indeed holds, we prove
another Hoare triple with weaker precondition
and stronger postcondition that is suitable for a proof
of the recursive calls.
In other words this contract (which we call the generalized contract for \MyMLe{span}) pertains to the intermediate
states which the program can be in due to the recursive call.

Since the recursive calls to \MyMLe{span} are executed
concurrently, the graph must be shared among different threads.
We will therefore distinguish between the global knowledge
about the graph in memory and the (thread) local knowledge
which pertains to the part of the graph that is marked by the
current thread.
We use $\globmark$ to denote the set of nodes that
are marked in the graph in memory. 
In order to specify and prove the correctness of \MyMLe{span} we will use
a protocol to enforce correct updates to the graph in memory.
Here we write $\globprot(g)$ to say that we have a protocol over the memory containing the graph:
\[
\globprot(g) = \exists \mathcal{G}.~\mathcal{G} \strictSG g \star \inmem(\mathit{update}(g, \mathcal{G})) \star \globmark = \nodes(\mathcal{G})
\]
where $\mathit{update}(g, \mathcal{G})$ is a graph with nodes of $g$ such that for any node $x$ we have:
\[
\mathit{update}(g, \mathcal{G}).x.\Left = \left\{
\begin{array}{ll}
\mathcal{G}.x.\Left &\text{if } x \in \nodes(\mathcal{G})\\
g.x.\Left &\text{otherwise}
\end{array}
\right.
\]
\[
\mathit{update}(g, \mathcal{G}).x.\Right = \left\{
\begin{array}{ll}
\mathcal{G}.x.\Right &\text{if } x \in \nodes(\mathcal{G})\\
g.x.\Right &\text{otherwise}
\end{array}
\right.
\]
In prose the protocol $\globprot(g)$ reads; there is a graph $\mathcal{G}$ representing the part of the graph that is marked
(globally across all threads) such that $\mathcal{G} \strictSG g$,
the memory contains the graph $g$ when updated with
information in $\mathcal{G}$, i.e., $\inmem(\mathit{update}(g, \mathcal{G}))$, and that
the set of graph nodes marked in the memory is the set of nodes of $\mathcal{G}$, i.e., $\globmark = \nodes(\mathcal{G})$.
We use $\localgr(G)$ to say that $G$ is a graph representing
the (thread) local information, i.e., the part of the graph
(original graph which the protocol is stated about) that is marked by the current thread.
For this, we have the following holding:
\[
\localgr(G \uplus G') \dashv\vdash \localgr(G) \star \localgr(G')
\]
where $\uplus$ is disjoint union of graphs (nodes must be disjoint) and $\dashv\vdash$ is logical equivalence.
The generalized contract for \MyMLe{span} is as follows:
\begin{align*}
& \{\globprot(g) \star \localgr(\emptyset)
\star x \in \{\text{\MyMLe{null}}\} \cup \nodes(g)
\star \connected(g, x)
\} \\
& \text{\MyMLe{span}}~x \\
& \left\{
\begin{array}{l@{}l}
v. &\big( v = \text{\MyMLe{true}} \star x \neq \text{\MyMLe{null}}
\star \exists G.~\localgr(G) \star \tree(G, x) \star\\
& \hspace{1em} \front(g, \nodes(G), \globmark) \star \\
& \hspace{1em} x \in \globmark \star \maximal(g) \big)\\
& \lor\\
& \big(
v = \text{\MyMLe{false}}
\star (x = \text{\MyMLe{null}} \lor x \in \globmark)
\star \localgr(\emptyset)
\big)
\end{array}
\right\}
\end{align*}
\subsection{Establishing the global contract based on the local contract}
We will use the rule of consequence of Hoare logic.
We need to establish the precondition of the generalized contract
from the precondition of the global one.
Since we have just started the thread, we know that
$\localgr(\emptyset)$.
Apart from this, we also have to establish the global protocol $\globprot(g)$. To this end, we will take $\mathcal{G}$
to be the empty graph.

Next, we need to show that the postcondition of the generalized contract
implies that of the global contract.
Since, from the precondition of the global contract we know that
$x \in \nodes(g)$ and also that nothing is marked initially we know that
the return value must be \MyMLe{true}.
In that case, we know that there is a graph $G$ such that
$\tree(G, x)$ and $\front(g, \nodes(G), \globmark)$.
We take $g'$ (in the postcondition of the global contract)
to be $G$. Since, the protocol states that
$\inmem(\mathit{update}(g, \mathcal{G}))$ and
$\mathit{update}(g, \mathcal{G}) = \mathcal{G}$ whenever $\nodes(g) = \nodes(\mathcal{G})$,
we only need to prove the latter to show the postcondition of
the global contract.
Notice that we have as part of the post condition of the generalized contract $\front(g, \nodes(G), \globmark)$ and
from the protocol we know that $\globmark = \nodes(\mathcal{G})$.
On the other hand, this is the top-level call and the graph was not initially marked.
Therefore, since $G$ is the part of the graph 
that this call has marked it must coincide with $\mathcal{G}$,
the graph that is marked globally, i.e., $G = \mathcal{G}$.
Hence, by Lemma~\ref{lem:in_front_nodes}, we have
$\nodes(g) = \nodes(G)$.

\subsection{Establishing the local contract}
Let us first consider the cases where
$x = \text{\MyMLe{null}}$ or \MyMLe{try_mark $x$} fails.
In both cases, we return \MyMLe{false} and the postconditions follows rather trivially. 
Otherwise, \MyMLe{try_mark $x$} has succeeded.
Notice that marking a node doesn't violate the protocol.
It merely extends $\mathcal{G}$ with node $x$.
In this case we have $x \in \globmark$ as well as
the local knowledge $\localgr(G)$ where $G$
is the graph consisting of only node $x$.
Notice that now the graph $G$ is not necessarily maximal as its left
and right children (should they exist) are not in $G$ itself.
We now run \MyMLe{span} concurrently on $x.\Left$ and $x.\Right$.
We need to establish the precondition for both children.
The protocol obviously holds, We also have that $G = G \uplus \emptyset \uplus \emptyset$ which implies
$\localgr(G) \dashv\vdash \localgr(G) \star \localgr(\emptyset) \star \localgr(\emptyset)$.
The rest of the precondition follows trivially.
After these calls have finished, if they fail (due to the child being
\MyMLe{null} or it being already marked), we remove the child anyway.
Notice that removing children does not violate the protocol.
It merely updates $\mathcal{G}$ but preserves the $\strictSG$ condition.
Assuming that we get $\localgr(G_l)$ and $\localgr(G_r)$ back from
the recursive calls on the left and right child respectively,
we need to show the post condition for
$\localgr(G \uplus G_l \uplus G_r)$.
It follows rather easily from $\tree(x.\Left, G_l)$ and
$\tree(x.\Right, G_r)$ that $\tree(x, G \uplus G_l \uplus G_r)$.
From $\maximal(G_l)$ and
$\maximal(G_r)$ it follows that $\maximal(G \uplus G_l \uplus G_r)$.
Also, it follows from $\front(g, \nodes(G_l), \globmark)$,
$\front(g, \nodes(G_r), \globmark)$,
$x_l \in \globmark$ and $x_r \in \globmark$ that
$\front(g, \nodes(G \uplus G_l \uplus G_r), \globmark)$.

\section{Proof in Iris}
In Iris, we formalize a graph as a map from nodes to its its children, i.e., as a finite map from memory locations $\mathit{loc}$ to pairs of optional locations:
\begin{Coq}
Definition graph := gmap loc (option loc * option loc).
\end{Coq}
Hence $\nodes(g)$ is written as \Coqe|dom (gset _) $g$|, which is the domain of the finite map expressed as a finite set (\Coqe|gset|).
The type of a node in the programming language%
\footnote{Iris is language agnostic and different languages can be formalized on top of it.
Here we use the one that is defined as part of Iris itself.}
of Iris is $\Tref(\Tref(\Tbool) \times (\locT \times \locT))$.%
\footnote{Notice that the language that we work with is not typed.
The type given here is just a for ease of representation.}
Here the first boolean indicates whether the node is marked or not.
It is crucial for the correctness of our algorithm that the reference to this boolean value
always remains the same (its value may change but not its physical location).
Therefore, we use a map, \Coqe|markings|, that maps each node (location) to a location that contains the marking for that node.

The main theorem proven in Iris is \Coqe{wp_span} below
which is precisely the global contract we discussed above.
In Iris, the Hoare triple $\{P\}e\{Q\}$ is written as $P \vdash \wpre{e}{Q}$
where $\wpre{e}{Q}$ is the weakest precondition for
$e$ with postcondition $Q$.
\begin{Coq}
Lemma wp_span g markings (l : loc) :
    l $\in$ dom (gset _) g -> maximal g -> connected g l ->
    heap_ctx $\star$
    ([$\star$ map] l $\mapsto$ v $\in$ g,
       $\exists$ (m : loc), markings !! l = Some m $\star$
          l $\mapsto$ ($\#$m, children_to_val v) $\star$ m $\mapsto$ $\#$false) $\vdash$
    WP span (SOME $\#$l)
    {{ _, $\exists$ g',
            ([$\star$ map] l $\mapsto$ v $\in$ g',
              $\exists$ m : loc, markings !! l = Some m $\star$
                l $\mapsto$ ($\#$m, children_to_val v) $\star$ m $\mapsto$ $\#$true)
           $\star$ dom (gset _) g = dom (gset _) g'
           $\star$ $\scriptscriptstyle\blacksquare$ strict_subgraph g g' $\star$ $\scriptscriptstyle\blacksquare$ tree g' l }}.
\end{Coq}
Here $\star$ is the separating conjunction. The symbol $\scriptscriptstyle\blacksquare$ is notation for the modality that
embeds pure propositions (those of type \Coqe|Prop| in Coq) into
the separation logic language of Iris (the equality propositions are pure but they are treated specially in parsing so that this modality is applied without any syntactical cue).
The Iris proposition \Coqe|heap_ctx| is an invariant that relates the heap
assertions, e.g., the points-to assertion $\mapsto$, to the physical memory.
The notation \Coqe|[$\star$ map] x $
\mapsto$ v $\in$ g, P | for a finite map \Coqe|g| where \Coqe|x| and \Coqe|v| are free in \Coqe|P| is a Coq notation for $
\bigstar_{x \in dom(g)} P(x, g(x))$.
Similarly, the notation \Coqe|[$\star$ map] x $\in$ A, P | is the separating
conjunction over a finite set $A$.
The symbol $\#$ is a Coq notation. It is a part of the programming
language defined to write programs in Iris.
It internalizes literals like locations and booleans from Coq types to the literal terms in the Iris language.
The infix operator \Coqe|!!| is the lookup operator for finite maps.

\subsection{The generalized contract for \Coqe|wp_span|}
The generalized contract for \MyMLe{span}, \Coqe{rec_wp_span}, is as follows.
\begin{Coq}
Lemma rec_wp_span g markings k q (x : val) :
    maximal g ->
    (x = NONEV $\lor$ $\exists$ l : loc,
        x = SOMEV $\#$l $\land$ l $\in$ dom (gset _) g) ->
    (heap_ctx $\star$ graph_ctx $\kappa$ g markings $\star$ own_graph q $\emptyset$ $\star$ cinv_own $\kappa$ k)
      $\vdash$
      WP (span x)
      {{ v, ((v = $\#$ true $\star$
             ($\exists$ l : loc, x = SOMEV $\#$l $\star$
               ($\exists$ (G : Gmon) mr (tr : tree (Gmon_graph G) l),
                  own_graph q G $\star$ $\scriptscriptstyle\blacksquare$ (l $\in$ dom (gset _) G) $\star$
                  $\scriptscriptstyle\blacksquare$ maximal (Gmon_graph G) $\star$
                  $\scriptscriptstyle\blacksquare$ (front g (dom (gset _) G) mr) $\star$
                   ([$\star$ set] l $\in$ mr , is_marked l)) $\star$ is_marked l)) $\lor$
             (v = $\#$ false $\star$
              (x = NONEV $\lor$ ($\exists$ l : loc, x = SOMEV $\#$l $\star$ is_marked l))
               $\star$ own_graph q $\emptyset$))
            $\star$ cinv_own $\kappa$ k }}.
\end{Coq}

This generalized contract in Coq is more or less the same as the intuitive one above.
The differences are $\kappa$ in \Coqe|graph_ctx|, \Coqe|cinv_own $\kappa$ k|
and $q$ in \Coqe|own_graph|.
Also, the concept of $\globmark$ is expressed
as a predicate \Coqe|is_marked $l$| to stand for  $l \in \globmark$.
In the following we will explain how these concepts are formalized in Iris.

\subsubsection{The protocol in Iris}
Iris uses invariants to enforce protocols.
An invariant $\knowInv{N}{P}$ expresses the fact that $P$ always holds.
We can use the invariant using the following rules.%
\footnote{These rules also involve the later modality as the logic of Iris is step indexed.
We have omitted the later modality here as it is orthogonal to our discussion.}
\begin{mathpar}
\inferH{Inv-alloc}{I \vdash \knowInv{N}{I}}{}
\and
\inferH{Inv}{\knowInv{N}{I} \star I \vdash \wpre{e}{I \star Q}{_{\mathcal{E} \setminus N}} \and
\mathit{physically\text{-}atomic(e)} \and
N \in \mathcal{E}}
{\knowInv{N}{I} \vdash \wpre{e}Q{_\mathcal{E}}}
\end{mathpar}
In practice weakest preconditions always have a mask of
the invariants that are opened.
This is to prevent an invariant be opened multiple times,
which in general is not sound.
Whenever the mask is $\mathcal{E} = \top$ (no invariant is open) we omit it.
As the name suggests, once an invariant is established it should always (invariantly) hold.
However, in our intuitive reasoning, we assumed that the
protocol $\globprot$ owns the part of the memory where the graph is in.
Using an invariant in this case would mean that we would not be able to
establish the postcondition of the global contract as we cannot
relinquish the ownership of the graph in the heap from the protocol.
To solve this problem, Iris features what we call cancellable invariants.
These are invariants that can be violated safely when we know no-one can use them anymore.
Written $\knowInv{N, \kappa}{P}$, a cancellable invariant can only be opened if
in addition to knowing that the invariant holds
we also own a fraction $k$ of the key $\kappa$ to open the invariant.
The rules for using cancellable invariants are as follows.
\begin{mathpar}
\inferH{CInv-alloc}{I \vdash \knowInv{N, \kappa}{I} \star \cinvown(\kappa, 1)}{}
\and
\inferH{Cinv-Key-Divide}{\cinvown(\kappa, k + k') \dashv\vdash
\cinvown(\kappa, k) \star \cinvown(\kappa, k')}{}
\and
\inferH{CInv}{\knowInv{N}{I} \star \cinvown(\kappa, k)
\star I \vdash \wpre{e}{I \star Q}{_{\mathcal{E} \setminus N}} \and
\mathit{physically\text{-}atomic(e)} \and
N \in \mathcal{E}}
{\knowInv{N}{I} \star \cinvown(\kappa, k) \vdash \wpre{e}Q{_\mathcal{E}}}
\and
\inferH{CInv-cancel}{\knowInv{N, \kappa}{I} \star \cinvown(\kappa, 1) \vdash \knowInv{N, \kappa}{I} \star I}{}
\end{mathpar}

We will later explain \Coqe|graph_ctx $\kappa$ g markings| and
\Coqe|cinv_own $\kappa$ k|. For now, they do not matter in our
arguments.

The protocol for \Coqe{graph_ctx $\kappa$ g} is intuitively defined as follows:
\[
\globprot(\kappa, g) \eqdef
\knowInv{N, \kappa}
{
\begin{array}{l@{}l}
\exists \mathcal{G}.&\globgr(\mathcal{G})
\star
\inmem(update(g, \mathcal{G}))
\star\\
& \globmr(\nodes(\mathcal{G}))
\star
\mathcal{G} \strictSG g
\end{array}
}
\]
In the following we will make the predicates $\globgr$ and $\globmr$ clear.
For now, it suffices to say that
$\globgr(\mathcal{G}) \star \localgr(g) \vdash g \subseteq \mathcal{G}$.
Notice that $g \subseteq \mathcal{G}$ states that
$g$ is exactly a part of $\mathcal{G}$ and not a strict subgraph as we have defined.
We also have that $\globmr(A) \star \mathit{is\text{-}marked}(l) \vdash l \in A$.

\subsubsection{Monoids: ghost state}
Iris uses partial commutative monoids (here, monoid for short) to represent its ghost states.%
\footnote{Ghost state in Iris is in fact a bit more general, i.e., they are represented as resource algebras (RAs) but this is orthogonal to our discussion here.}
One can compose monoids to build larger monoids.
We write $\ownGhost{\gamma}{a}$ to indicate that we own the ghost state corresponding to the monoid element $a$ at the monoid instance $\gamma$ ($\gamma$ is called the monoid name).
The monoids can shared between different threads using the rule
\[
\ownGhost{\gamma}{a \cdot b} \dashv\vdash \ownGhost{\gamma}{a} \star \ownGhost{\gamma}{b}
\]
where $\cdot$ is the monoid operation.

Instances of Monoids are allocated using the following rule.
\begin{mathpar}
\inferH{Monoid-alloc}{a \neq \bot \vdash \exists \gamma.~\ownGhost{\gamma}{a}}{}
\end{mathpar}
They can be update using the following rules
\begin{mathpar}
\inferH{Monoid-update}
{a \mupd b}{\ownGhost{\gamma}{a} \vdash \ownGhost{\gamma}{b}}
\and
\inferH{FP-update}{\forall \f.~ a \cdot f \neq \bot \Rightarrow
b \cdot f \neq \bot}{a \mupd b}
\end{mathpar}

In Iris, in order to express as ghost state the global and (thread) local
ownership of ghost state, we have a monoid construct called the
authoritative monoid, $\Auth(M)$, where $M$ is a monoid.
For the definition of the authoritative monoid, see Appendix~\ref{sec:app-mon}.
The elements of $\Auth(M)$ are generated by the authoritative element
$\authfull a$ for $a \in M$ and the fragment element $\authfrag a$ for $a \in M$
which, express global and local ownership of the ghost states of monoid $M$.
This can be seen in the following rules.
\begin{mathpar}
\infer{\authfull a \cdot \authfull b = \bot}{}
\and
\infer{\ownGhost{\gamma}{\authfull a \cdot \authfrag b} \vdash b \preceq a}{}
\and
\infer{\authfrag a \cdot \authfrag b = \authfrag (a \cdot b)}{}
\end{mathpar}
where $a \preceq b$ is the extension order for the monoid $M$, i.e., $a \preceq b \eqdef a = b \lor \exists c.~ b = a \cdot c$.

In practice, we are going to use the authoritative monoid over some monoid to define $\globprot$, we are going to use the authoritative part
of the monoid $\authfull a$ to represent $\mathcal{G}$, the part of the graph
that is globally marked.

In our intuitive reasoning above where we explained why the global contract
follows from the generalized contract, we argued that since
that call was the top-level call, the part of the graph marked globally
as described by the protocol should coincide with the
part that was marked by that call.
However, in Iris, we can't distinguish and say that
a contract corresponds to the top-level call of the
code.
On the other hand, having
$\ownGhost{\gamma}{\authfull a \cdot \authfrag b = \bot}$
we can only conclude $b \preceq a$ and not $a = b$ as we need.
Unless we can somehow show that there is no such $c$ that $b = a \cdot c$.
Therefore, we define the monoid for representing graphs as follows:
\[
\graphm \eqdef \locT \fpfn \excl(\Val \times \Val)
\]
\[
\gmon \eqdef \fracm \times \graphm
\]
It is constructed using the product monoid constructor.
Here $A \fpfn M$ is the finite map monoid mapping
elements of type $A$ to elements of monoid $M$. 
This closely follows the way we defined graphs in Coq.
The exclusive monoid $\excl(M)$ has as elements $\Excl(a)$
for $a \in M$ such that $\Excl(a) \cdot \Excl(b) = \bot$.
In other words, we can't distribute the information about children
of a node.
The monoid $\fracm$ is the monoid of fractions with elements
in the subset $(0, 1] \subseteq \mathbb{Q}$ of the rational numbers
with the monoid operation:
\[
q \cdot q' =
\left\{
\begin{array}{ll}
q + q' & \text{if } q + q' \le 1\\
\bot &\text{otherwise}
\end{array}
\right.
\]
This means that for the graph monoid $\gmon$ we have:
\begin{mathpar}
\infer{q + q' \le 1 \and G \cdot G' \neq \bot}{(q, G) \cdot (q', G') = (q + q', G \cdot G')}
\and
\infer{\ownGhost{\gamma}
{\authfull (1, G) \cdot \authfrag (1, G')} \vdash G = G'}{}
\end{mathpar}
Notice that due to the use of the exclusive monoid the operation on
$\graphm$ is simply disjoint union which corresponds to what we used
in our intuitive reasoning.

We use the monoid $\markings$ to define both $\globmark$ and $\ismarked$:
\[
\markings \eqdef \authm(\finset(\locT))
\]
where \finset(A) for a type $A$ is the monoid of finite subsets of $A$
with the monoid operation being union.

Now we define the protocol $\globprot$, $\localgr$, $\globmark$ and $\ismarked$.
Here $\grname$ and $\mrname$ are respectively instance names 
for monoids corresponding to graph and markings monoids.
\[
\globprot(\kappa, g) \eqdef
\knowInv{N, \kappa}
{
\begin{array}{l@{}l}
\exists \mathcal{G}.& \ownGhost{\gamma_{gr}}{\authfull (1, \mathcal{G})}
\star
\inmem(update(g, \mathcal{G}))
\star\\
& \ownGhost{\mrname}{\authfull \nodes(\mathcal{G})}
\star
\mathcal{G} \strictSG g
\end{array}
}
\]
\[
\localgr(q, G) \eqdef \ownGhost{\grname}{\authfrag (q, G)}
\]
\[
\ismarked(a) \eqdef \ownGhost{\mrname}{\authfrag \{a\}}
\]

\subsubsection{Proof in Iris}
The proof of the generalized and global contract for
\MyMLe{span} follow very closely the intuitive argument that we gave earlier.
It is carried out using Iris and its proof mode
by symbolic execution of the program.

The only difference is that when we divide the
graph monoid, we use
\[
\localgr(q, G) \dashv\vdash \localgr(\sfrac{q}{2}, G) \star \localgr(\sfrac{q}{4},\emptyset) \star \localgr(\sfrac{q}{4},\emptyset)
\]
Also, the key to the cancellable invariant is divided in half
to be given to the two recursive calls.

We leave the proof of the contracts for
\MyMLe{try_mark},
\MyMLe{unmark_fst} and \MyMLe{unmark_snd}
to the reader.
Here \Coqe{(g !! x) = Some v} simply states that the children of node \Coqe{x}
in graph \Coqe{g} are represented by the value \Coqe{v}.
\begin{Coq}
Lemma wp_try_mark g markings k q (x : loc) : x $\in$ dom (gset _) g ->
    heap_ctx $\star$ graph_ctx $\kappa$ g markings $\star$ own_graph q $\emptyset$
     $\star$ cinv_own $\kappa$ k
    $\vdash$ WP (try_mark $\#$x) {{ v,
         (v = $\#$true $\star$ ($\exists$ u, (g !! x) = Some u $\star$ own_graph q (x [$\mapsto$] u))
          $\star$ is_marked x $\star$ cinv_own $\kappa$ k)
           $\lor$ (v = $\#$false $\star$ own_graph q $\emptyset$ $\star$ is_marked x
              $\star$ cinv_own $\kappa$ k) }}.
\end{Coq}

\begin{Coq}
Lemma wp_unmark_fst g markings k q (x : loc) v w1 w2 :
    (g !! x) = Some v ->
    (heap_ctx $\star$ graph_ctx $\kappa$ g markings
     $\star$ own_graph q (x [$\mapsto$] (w1, w2)) $\star$ cinv_own $\kappa$ k) $\vdash$
      WP (unmark_fst $\#$x)
      {{ _, own_graph q (x [$\mapsto$] (None, w2)) $\star$ cinv_own $\kappa$ k }}.
\end{Coq}

\begin{Coq}
Lemma wp_unmark_snd g markings k q (x : loc) v w1 w2 :
    (g !! x) = Some v ->
    (heap_ctx $\star$ graph_ctx $\kappa$ g markings
    $\star$ own_graph q (x [$\mapsto$] (w1, w2)) $\star$ cinv_own $\kappa$ k) $\vdash$
      WP (unmark_snd $\#$x)
      {{ _, own_graph q (x [$\mapsto$] (w1, None)) $\star$ cinv_own $\kappa$ k }}.
\end{Coq}

\section{Comparison}
The intuitive reasoning of the proof that we discussed above is inspired by that of \cite{Sergey:2015:MVF:2737924.2737964}.
The main differences in the implementation of these ideas come from the
systems they are implemented in: Iris in our case and FCSL in the case of
\cite{Sergey:2015:MVF:2737924.2737964}.
Here in Iris, we used monoids and invaraints to model ghost state and
enforce protocols.
In FCSL, the authors use the notion of a concurroid.

\subsection{Ghost state}
A concurroid is in practice a state-transition system (STS for short)
enforcing a rely-guarantee style protocol on threads.
In this case, the states of the STS the authors of \cite{Sergey:2015:MVF:2737924.2737964} use is a partitioning of the
heap corresponding to the graph into three different parts:
\textit{self}, \textit{joint} and \textit{other}.
Intuitively, \textit{self} is the part that the current thread has marked,
\textit{joint} is the part that is shared between threads (not yet marked)
and \textit{other} is the part that is marked by other threads.
There are then transitions on these states that correspond to marking
(which moves a node from \textit{joint} to \textit{self}) and removing
edges (to the left or right child) which removes the left/right child of a
node in \textit{self}.

Any assertion that is used in a proof of the correctness of the program
using a concurroid, e.g., those that appear in the pre-condition and post-condition,
need to be shown to be stable.
That is, any such property, e.g., the fact that a node is marked, must be
shown to be stable under interference from other threads, i.e., with
respect to the concurroid transitions taken by other threads.

In Iris, we do not have the concept of stability.
On the other hand, when we prove that some ghost resource can be
updated to another we need to show that this does not violate other
threads' knowledge of that ghost resource
(see the rules \textsc{Monoid-update} and \textsc{FP-update}), e.g., marking is only possible
if the node is not already marked -- which we get from the fact that the \MyMLe|CAS| in \MyMLe|try_mark| succeeds.

In this regard, the approaches of Iris and FCSL are dual.
In Iris ghost state can be updated only when it doesn't violate the knowledge
of the other threads while in FCSL any update (transitions of STS) is valid
and it is the properties that need to be shown to be stable under these transitions.
There are two important aspects that when put together make the approach taken by Iris
more convenient to work with.

\paragraph{Monoids versus heap}
In FCSL, ghost states are simply states of an STS.
Each STS comes with an interpretation which maps states to parts of the heap.
This means that proving everything in FCSL, including stability, requires working directly
with the heap.
Therefore, in order to prove stability of properties one has to reason about the
parts of heap that states represent and show that under any transition
(that any other thread might take) the property in question persists to hold.
As a result, it is hard, if not impossible to generalize these stability arguments.
This is perhaps the reason why in \cite{Sergey:2015:MVF:2737924.2737964} each and every stability lemma is proven from scratch.

In Iris, by using monoids, not only do we avoid the hassle of working directly with the heap,
but also have the ability to generalize a lot of the conditions necessary for the update lemmas.
In fact, in Iris, one hardly ever has to prove non-trivial update lemmas.
This is mainly due to the fact that the proof of update lemmas, much the same way
as the construction of monoids out of monoid constructors, can be easily proven using the
update lemmas for those monoid constructors which are already available as part of the Iris library.

\paragraph{Number of transitions versus number of properties}
Another important factor differentiating the two approaches is that  usually the number of
updates (STS transitions in FCSL's case) is smaller than the numebr of properties that
are required to express the correctness of the algorithm.
Even in this not-so-simple algorithm we only have two updates corresponding
to marking and removing the left/right child.

In Iris, we only prove two lemmas for these updates: one for marking nodes and one for
updating children of a node.
In FCSL, the authors prove stability for properties for: \Coqe|edge|,
\Coqe|mark|, \Coqe|unmarked| and \Coqe|dom| which correspond a node having an edge
to another, a node being marked, a node being unmarked and the domain of the heap
corresponding to the \textit{joint} region.
In some cases, there are more than one stability lemmas proven for the same property.

\subsection{The top-level call}
A crucial part in proving the correctness of this program is the
assumption that when the top-level call to \MyMLe{span} happens there are
no other threads running \MyMLe|span|, i.e., it is indeed the top-level call.

In FCSL, this is enforced by an operation \textit{hide} which given a
concurroid and a program runs the program assuming there are no
other threads with access to the state managed by that concurroid.

In Iris, there is a clear distinction between the logic on one hand and the programming
language and the program logic on the other.%
\footnote{This is why Iris and be instantiated with different programming languages.}
The logic is a separate entity and can be used to reason about ghost states and invariants
which are inherently separate from any program, programming language or program logic.
Hence, it does not feature any counterpart to the \textit{hide} operation of FCSL.
On the other hand, there is a canonical way in Iris to reason about sharing of resources:
the monoid of fractions, $\Frac$.
This is indeed how the fractional heap chunks, which are prevalent in separation logic, are
defined in Iris.
This is why the graph monoid that we have used is, as explained, paired with a fraction.
Since the generalized contract for \MyMLe|span| gives back in its
post-condition the same fraction of a graph that it was given and at
the top-level it is called with the full fraction we can conclude that
when the top-level call to \MyMLe|span| finishes there are indeed no
other threads running that have access to the graph.

With fractions being an important and versatile monoid, there are many useful lemmas,
including suitable update lemmas, about them in the Iris library.
That is why using the fraction monoid along with the graph monoid does in noway add any inconvenience compared to working directly with the graph monoid.

\subsection{The size of the development}
The proof of this algorithm in FCSL is 1626 lines of Coq code%
\footnote{Number of lines of code mentioned for developments using FCSL are not from
\cite{Sergey:2015:MVF:2737924.2737964}.
These numbers are those obtained from their more recent versions which are available online that we could examine.}
while ours is 1509 lines.
This seems surprising as one would expect that working directly with the heap be more
labour intensive and more bloated.
On the other hand, \cite{Sergey:2015:MVF:2737924.2737964} being an exposition of
examples of program verification in FCSL, focuses mainly on this spanning tree algorithm and
only very briefly discusses the other examples formalized in FCSL.
Indeed this spanning tree algorithm is the most intricate of all other examples discussed in that paper.
What is more interesting is the fact that examples a lot simpler do not follow the same pattern:
CAS-lock being 1612 lines and ticketed-lock being 2143 lines.
We suspect that the reason for these differences stems from the fact that the structure of
a graph (as a mapping from nodes to their children) very closely resembles that of their heap
representation.
As a result, reasoning directly about the heap for such special cases is not as cumbersome
as for cases where the ghost state does not resemble the heap, e.g., a lock program.

\bibliographystyle{plain}
\bibliography{ref}

\appendix
\section{Monoids}\label{sec:app-mon}
The monoid former $\fpfn$ is constructs finite maps from a type to a monoid.
The composition operation $\cdot$ is defined as follows:
\[
(f \cdot g) (x) = \left\{
\begin{array}{ll}
f(x) & \text{if } x \in \dom(f)\setminus\dom(g)\\
g(x) & \text{if } x \in \dom(g)\setminus\dom(f)\\
f(x) \cdot g(x) & \text{if } x \in \dom(f)\cap\dom(g)\\
\bot & \text{otherwise}
\end{array}
\right.
\]
The monoid former $\option(A)$ for a monoid $A$ has as elements 
$\None$ or $\Some(x)$ for $x \in A$ with:
\[
\begin{array}{ll}
\None \cdot \None = \None & \None \cdot \Some(x) = \Some(x) \\
\multicolumn{2}{c}{\Some(x) \cdot \Some(y) = \Some(x \cdot y)}
\end{array}
\]

The monoid former of $\option(A)$ for a monoid $A$ has as elements 
$\None$ or $\Some(x)$ for $x \in A$ with:
\[
\begin{array}{ll}
\None \cdot \None = \None & \None \cdot \Some(x) = \Some(x) \\
\multicolumn{2}{c}{\Some(x) \cdot \Some(y) = \Some(x \cdot y)}
\end{array}
\]

The monoid former of $\Excl(A)$ for a \emph{type} $A$ has as elements 
$\excl(x)$ for $x \in A$ with:
\[
\begin{array}{ll}
a \cdot b = \bot
\end{array}
\]

The fraction monoid $\Frac$ has as elements rational numbers in the interval $(0, 1]$ and the operation is defined as:
\[
q \cdot q' = \left\{
\begin{array}{ll}
q + q' & \text{if } q + q' \le 1\\
\bot & \text{otherwise}
\end{array}
\right.
\]

The product monoid former $A \times B$ for monoids $A$ and $B$ has as elements pairs of elements of $A$ and $B$ and the operation is defined as:
\[
(a, b) \cdot (a', b') = (a \cdot a', b \cdot b')
\]

For the subsumption order (the extension order) for a moinoid $A$ we have that $a \preceq b$ defined as:
\[
a \preceq b \eqdef a = b \lor \exists c.~b = a \cdot c
\]
Notice that the non-standard definition of extension order here as
there might not exists a $c$ such that $b = a \cdot c$.
The prime example here is the subsumption relation $1 \preceq 1$ in
$\Frac$.
This is exactly why we can derive:
\[
(1, a) \preceq (1, b) \Rightarrow a = b
\]

The monoid former $\authm(M)$ has as elements the set
$\{\authfull a \mid a \in M\} \cup \{\authfrag a \mid a \in M\}$
closed under the monoid operation described below.
\[
\authfull a \cdot \authfull b = \bot
\hspace{2em}
\authfull a \cdot \authfrag b =
\left\{
\begin{array}{ll}
\authfull a \cdot \authfrag b & \text{if } b \preceq a\\
\bot & \text{otherwise}
\end{array}
\right.
\hspace{2em}
\authfrag a \cdot \authfrag b =
\left\{
\begin{array}{ll}
\authfrag (a \cdot b) & \text{if } a \cdot b \neq \bot\\
\bot & \text{otherwise}
\end{array}
\right.
\]



\end{document}